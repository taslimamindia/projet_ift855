from fireworks import LLM
from outils.dataset import Data

class Fireworks_LLM:
    def __init__(self, data:Data, model="llama4-maverick-instruct-basic", deployment_type="serverless"):
        """Connect to an LLM model via the Fireworks API.

        Args:
            model (str, optional): Name of the LLM model. Defaults to "llama4-maverick-instruct-basic".
            deployment_type (str, optional): Deployment type. Defaults to "serverless".
            fireworks_api_key (str, optional): Fireworks API key. Defaults to "fw_3Zg5B7CUKag67HsSZjCwwbwx".
        """
        
        self.data = data
        self.llm = LLM(model=model, deployment_type=deployment_type, api_key=self.data.fireworks_api_key)


    def generate_answer(self, query=None, context=None, last_response=None):
        """Generate an answer using the LLM model.

        Args:
            query (str, optional): User query. Defaults to None.
            context (str, optional): Relevant context for the query. Defaults to None.
            last_response (str, optional): Last response generated by the model. Defaults to None.

        Returns:
            str: Generated response content from the LLM.
        """
        
        prompt = []

        if context:
            prompt.append({"role": "system", "content": context})

        if last_response:
            prompt.append({"role": "assistant", "content": last_response})

        if query:
            prompt.append({"role": "user", "content": query})
    
        response = self.llm.chat.completions.create(messages=prompt)
        return response.choices[0].message.content


    def generate_QA(self, query=None, context=None, last_response=None):
        """Generate an answer using the LLM model.

        Args:
            query (str, optional): User query. Defaults to None.
            context (str, optional): Relevant context for the query. Defaults to None.
            last_response (str, optional): Last response generated by the model. Defaults to None.

        Returns:
            str: Generated response content from the LLM.
        """

        constraints = """
            Contraintes: Tu es en mode assistant qui répond sous forme de questions-réponses.
            - Les réponses doivent être directes et précises.
            - Ne donne pas plus de détails que ce qui est demandé.
        """

        constraints = " ".join([constraint.strip() for constraint in constraints.split("\n")])
        prompt = [{"role": "system", "content": constraints}]

        if context:
            prompt.append({"role": "system", "content": context})

        if last_response:
            prompt.append({"role": "assistant", "content": last_response})

        if query:
            prompt.append({"role": "user", "content": query})

        response = self.llm.chat.completions.create(messages=prompt)
        return response.choices[0].message.content